{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\", context=\"talk\")\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "from torch.utils.data import dataloader, dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import f1_score, mean_squared_error, roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import KFold \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./dataset/train.csv.zip', nrows=200)\n",
    "test = pd.read_csv('./dataset/test.csv.zip', nrows=200)\n",
    "test_label = pd.read_csv('./dataset/test_labels.csv.zip', nrows=200)\n",
    "train.head()\n",
    "#test.head()\n",
    "#test_label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the dataset -- helper function for removing non-sense fragment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    #pattern = [zero or more character]\n",
    "\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    #pattern = removes (http),://, 'and' www.\n",
    "    \n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    #pattern = any punctionation\n",
    "\n",
    "    text = re.sub('\\n', '', text)\n",
    "    #pattern = any new line\n",
    "\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    #pattern = any from[a-zA-Z0-9_], any from[0-9], any from [a-zA-Z0-9_]\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "train['clean'] = train['comment_text'].apply(str).apply(lambda x: clean_text(x))\n",
    "test['clean'] = test['comment_text'].apply(str).apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataSet(dataset.Dataset):\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_len, if_train=True):\n",
    "        self.texts = texts\n",
    "        #self.labels = labels.to_numpy()\n",
    "        self.if_train = if_train\n",
    "        if self.if_train:\n",
    "            self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts.iloc[item])\n",
    "        \n",
    "        if self.if_train:\n",
    "            label = self.labels.iloc[item]\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=True,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        if self.if_train:\n",
    "            return {\n",
    "                #'text': text,\n",
    "                'input_ids':torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "                'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
    "                'labels': torch.tensor(label, dtype=torch.float)\n",
    "            }\n",
    "        else:\n",
    "            return{\n",
    "            'input_ids':torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "            'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long)\n",
    "            }\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#senten_len = []\n",
    "#for sentence in tqdm(train['clean']):\n",
    "#    token_words = tokenizer.encode_plus(sentence, padding=True, max_length = 768)['input_ids'] # Token indices sequence length is longer than the specified maximum sequence length for this model (569 > 512). Running this sequence through the model will result in indexing errors\n",
    "#    senten_len.append(len(token_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = next(iter(train['clean']))\n",
    "print(type(train['clean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = 32\n",
    "valid_batch = 32\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "print(f\"The devices using is : {device}\")\n",
    "\n",
    "#Using K-folder\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# fold_index = 0\n",
    "\n",
    "fold_losses = []\n",
    "fold_valid = []\n",
    "\n",
    "best_model  = None\n",
    "best_loss = None\n",
    "best_index = 0\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss() # for sigmoid on multi-label tasks\n",
    "loss_fn.to(device)\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = transformers.BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels = 6)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, valid_index in kf.split(train):\n",
    "    train_kf = train.iloc[train_index]\n",
    "    valid_kf = train.iloc[valid_index]\n",
    "    \n",
    "    train_dataset = BertDataSet(train_kf['clean'], train_kf[['toxic', 'severe_toxic','obscene', 'threat', 'insult','identity_hate']], tokenizer, 77)\n",
    "    valid_dataset = BertDataSet(valid_kf['clean'], valid_kf[['toxic', 'severe_toxic','obscene', 'threat', 'insult','identity_hate']], tokenizer, 77)\n",
    "    \n",
    "    #train_dataset = BertDataSet(train_kf, tokenizer, eval_mode = False)\n",
    "    #valid_dataset = BertDataSet(valid_kf, tokenizer, eval_mode = True)\n",
    "    \n",
    "    train_dataloader = dataloader.DataLoader(train_dataset, batch_size = train_batch, pin_memory = True, num_workers = 0, shuffle = True)\n",
    "    valid_dataloader = dataloader.DataLoader(valid_dataset, batch_size = valid_batch, pin_memory = True, num_workers = 0, shuffle = False)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "    training_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):   \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        valid_loss = 0\n",
    "        with torch.cuda.amp.autocast():\n",
    "     \n",
    "            for batch in tqdm(train_dataloader):\n",
    "                \n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels = labels)\n",
    "                outputs = outputs['logits']\n",
    "                \n",
    "                toxic_labels = batch['labels'].to(device, non_blocking=True)\n",
    "                loss = loss_fn(outputs, toxic_labels)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            avg_loss = total_loss / len(train_dataloader)\n",
    "            training_losses.append(avg_loss)\n",
    "            print(f\"Training Epoch {epoch+1}/{epochs}, Loss: {avg_loss}. {len(training_losses)}\")\n",
    "        \n",
    "            model.eval()\n",
    "            true_labels = []\n",
    "            predictions = []\n",
    "            \n",
    "            for batch in valid_dataloader:\n",
    "                \n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                        \n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model(input_ids, attention_mask = attention_mask)\n",
    "                    \n",
    "                outputs = outputs['logits'].squeeze(-1)\n",
    "                \n",
    "                toxic_labels = batch['labels'].to(device, non_blocking=True)\n",
    "                loss = loss_fn(outputs, toxic_labels)\n",
    "                valid_loss += loss.item()\n",
    "                \n",
    "            avg_valid_loss = valid_loss / len(valid_dataloader)\n",
    "            valid_losses.append(avg_valid_loss)   \n",
    "            #f1 = f1_score(true_labels, predictions, average='weighted')  #Error : Classification metrics can't handle a mix of multilabel-indicator and binary targets       \n",
    "            print(f\"Validating Epoch {epoch+1}/{epochs}, Loss: {avg_valid_loss}\")\n",
    "        \n",
    "        if( best_loss == None or avg_valid_loss > best_loss ):\n",
    "            \n",
    "            best_model = model\n",
    "            best_loss = avg_valid_loss\n",
    "            fold_losses = training_losses\n",
    "            fold_valid = valid_losses\n",
    "            print(\"Find the better model\")\n",
    "\n",
    "    \n",
    "best_model.save_pretrained('./fintune_bert')    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the losses of the best model\n",
    "best_train_losses = fold_losses\n",
    "best_valid_losses = valid_losses\n",
    "\n",
    "plt.plot(best_train_losses, color='navy')\n",
    "plt.plot(best_valid_losses, color = 'blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(best_train_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = None\n",
    "best_model = None\n",
    "model = None\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "#loading test_dataset\n",
    "test_dataset = BertDataSet(test['clean'], None, tokenizer, 77, if_train = False)\n",
    "test_dataloader = dataloader.DataLoader(test_dataset, batch_size = valid_batch, pin_memory = True, num_workers = 0, shuffle = True)\n",
    "\n",
    "config_path = './fintune_bert/config.json'\n",
    "model_path = './fintune_bert/model.safetensors'\n",
    "\n",
    "config = BertConfig.from_json_file(config_path)\n",
    "\n",
    "best_model = BertModel.from_pretrained(model_path, config=config).to(device)\n",
    "\n",
    "for test in test_dataloader:\n",
    "    \n",
    "    ids = test['input_ids'].to(device)\n",
    "    mask = test['mask'].to(device)\n",
    "    outputs = best_model(ids, mask)\n",
    "    \n",
    "    #print(outputs.keys())\n",
    "    #results = outputs['logits']\n",
    "    results = outputs['last_hidden_state']\n",
    "    #results = outputs\n",
    "    raw_predictions = torch.sigmoid(results)\n",
    "    \n",
    "    prediction = torch.where(raw_predictions > 0.5, -1, 0)\n",
    "    print(prediction)\n",
    "    #print(type(raw_predictions))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = torch.tensor(test_label[['toxic', 'severe_toxic','obscene', 'threat', 'insult','identity_hate']].values)\n",
    "print(ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
